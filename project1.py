# -*- coding: utf-8 -*-
"""Copy of Copy 44 Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UsAVBsFGTXnNG0vWps5m_3Cht6QQqZeG

# **[call all the basic libraries:]**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder , OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
from pandas.plotting import scatter_matrix
from sklearn.impute import SimpleImputer
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import GridSearchCV

"""# **start reading the training and testing data sets:**"""

train_df=pd.read_csv('/content/drive/MyDrive/train.csv')
train_df.head()

train_df.info()

dim=pd.read_csv('/content/drive/MyDrive/train.csv')
dim.info()

test_df=pd.read_csv('/content/drive/MyDrive/test.csv')
test_df.head()
test_df.info()

mean_d = dim['price'].mean()
print("Mean Value of Diamonds: $", mean_d)

dim.describe()

dim.describe(include='object')

dim.nunique()

"""# 3. **visualize** the data"""

dim.hist(figsize=(18,10))

sns.pairplot(dim, y_vars='price')

sns.pairplot(dim)

dim["cut"].value_counts() / len(dim)

corr_matrix = dim.corr()
corr_matrix

corr_matrix['price'].sort_values(ascending=False)

plt.figure(figsize = (12,8))
corr_matrix['price'].sort_values(ascending = False).plot(kind = 'bar')

plt.figure(figsize = (16,5))
heato=sns.heatmap(corr_matrix ,cmap='BrBG' ,annot=True )
heato.set_title('Correlation Heatmap', fontdict={'fontsize':25})

dim.plot.scatter(x='carat', y='price' ,figsize=(10,5))

dim.plot.scatter(x='z', y='price' ,figsize=(10,5))

d.plot.scatter(x='z', y='price' ,figsize=(10,5))

input_cat_columns = dim.select_dtypes(include = ['object']).columns.to_list()

for col in input_cat_columns:
    sns.catplot(x=col, y="price", kind="box", dodge=False, height = 5, aspect = 3,data=dim);

"""# **Removing the outliers:**

"""

Q1=dim['depth'].quantile(0.25)
Q3=dim['depth'].quantile(0.75)
IQR=Q3-Q1
idx=~((dim['depth']<(Q1 - 1.5*IQR)) | (dim['depth'] >(Q3 + 1.5*IQR)))
d1=dim[idx]
d1.info()

Q1x=dim['x'].quantile(0.25)
Q3x=dim['x'].quantile(0.75)
IQRx=Q3x-Q1x
idxx=(d1['x']>(Q1x - 1.5*IQRx)) & (d1['x'] <(Q3x + 1.5*IQRx))
dx=d1[idxx]
dx.info()

Q1y=dim['y'].quantile(0.25)
Q3y=dim['y'].quantile(0.75)
IQRy=Q3y-Q1y
idxy=(dx['x']>(Q1y - 1.5*IQRy)) & (dx['x'] <(Q3y + 1.5*IQRy))
dy=dx[idxy]
dy.info()

Q1z=dim['z'].quantile(0.25)
Q3z=dim['z'].quantile(0.75)
IQRz=Q3z-Q1z
idxz=(dy['z']>(Q1z - 1.5*IQRz)) & (dy['z'] <(Q3z + 1.5*IQRz))
dz=dy[idxz]
dz.describe()
# dz.info()

Q1ca=dim['carat'].quantile(0.25)
Q3ca=dim['carat'].quantile(0.75)
IQRca=Q3-Q1
idxca=(dz['carat']>(Q1ca - 1.5*IQRca)) & (dz['x'] <(Q3ca + 1.5*IQRca))
dca=dz[idx]
dca.info()

Q1ta=dim['table'].quantile(0.25)
Q3ta=dim['table'].quantile(0.75)
IQRta=Q3ta-Q1ta
idxta=(dca['table']>(Q1ta - 1.5*IQRta)) & (dca['x'] <(Q3ta + 1.5*IQRta))
d=dca[idxta]
d.info()

dix=dim.drop('price',axis=1)
diy=dim['price']

x_train, x_test , y_train , y_test = train_test_split(dix, diy, test_size=0.25 , random_state=42)

# def prepare_data(df):
#   num_attribs=df.select_dtypes(include=[np.number]).columns.to_list()
#   num_pipeline = Pipeline([('std_scaler', StandardScaler())])
#   cat_attribs = ["color","clarity","cut"]
#   full_pipeline = ColumnTransformer([
#   ("num", num_pipeline, num_attribs),
#   ("cat", OneHotEncoder(), cat_attribs),
#    ])
#   data_prepared = full_pipeline.fit_transform(df)

num_attribs=x_train.select_dtypes(include=[np.number]).columns.to_list()
num_pipeline = Pipeline([('std_scaler', StandardScaler())])
cat_attribs = ["color","clarity","cut"]
full_pipeline = ColumnTransformer([
("num", num_pipeline, num_attribs),
("cat", OneHotEncoder(), cat_attribs),
])
train_prepared = full_pipeline.fit_transform(x_train)

test_prepared= full_pipeline.fit_transform(x_test)

# full_pipeline = ColumnTransformer([
# ("num", num_pipeline, num_attribs),
# ("cat", OneHotEncoder(), cat_attribs),
# ])

# cat_cols= d.select_dtypes(include='object').columns.to_list()
# dim1=pd.get_dummies(d , columns=cat_cols , drop_first=True)

# x=dim1.drop('price', axis=1)
# y=dim1['price']
# x_train, x_test , y_train , y_test = train_test_split(x,y, test_size=0.3 , random_state=42)

"""# **start Selecting and Training some Models**

**1. LinearRegression model**
"""

from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(train_prepared, y_train)

# some_data_prepared = full_pipeline.transform(some_data)
# print("Predictions:", lin_reg.predict(some_data_prepared))
# print("Labels:", list(some_labels))

dim1_predictions = lin_reg.predict(test_prepared)
lin_mse = mean_squared_error(y_test, dim1_predictions)
lin_rmse = np.sqrt(lin_mse)
lin_rmse

"""*** Using Cross-Validation***"""

lin_scores = cross_val_score(lin_reg, train_prepared, y_train, scoring="neg_mean_squared_error", cv=10)
lin_rmse_scores = np.sqrt(-lin_scores)

print("Scores: ", lin_rmse_scores)
print("Mean: ", lin_rmse_scores.mean())
print("Standard Deviation: ", lin_rmse_scores.std())

"""**2. Decision Tree Regressor model**

"""

tree_reg = DecisionTreeRegressor()
tree_reg.fit(train_prepared, y_train)

dimtree_predictions = tree_reg.predict(test_prepared)
tree_mse = mean_squared_error(y_test, dimtree_predictions)
tree_rmse = np.sqrt(tree_mse)
tree_rmse

"""*** Using Cross-Validation***"""

scores = cross_val_score(tree_reg, train_prepared, y_train, scoring="neg_mean_squared_error", cv=10)
tree_rmse_scores = np.sqrt(-scores)
print("Scores: ", tree_rmse_scores)
print("Mean: ", tree_rmse_scores.mean())
print("Standard Deviation: ", tree_rmse_scores.std())

"""**3. RandomForest Model:**"""

rand_for= RandomForestRegressor()
rand_for.fit(train_prepared, y_train)

ranfor_predictions = tree_reg.predict(test_prepared)
tree_mse = mean_squared_error(y_test, ranfor_predictions)
tree_rmse = np.sqrt(tree_mse)
tree_rmse

"""## Evaluation Models Using Cross-Validation

"""

forest_scores = cross_val_score(rand_for ,train_prepared, y_train,scoring = "neg_mean_squared_error", cv = 10)
forest_rmse_scores = np.sqrt(-forest_scores)
print("Scores: ", forest_rmse_scores)
print("Mean: ", forest_rmse_scores.mean())
print("Standard Deviation: ", forest_rmse_scores.std())

final_test=full_pipeline.fit_transform(test_df)

predictions=pd.Series(rand_for.predict(test_prepared))
pred=pd.DataFrame({'Id': test_df['Id'], 'price': predictions})

pred

"""# **Final Tune using Grid Search:**"""

from sklearn.model_selection import GridSearchCV

param_grid = [
              {'n_estimators': [3,10,30], 'max_features':[2,4,6,8]},
              {'bootstrap':[False], 'max_features':[2,3,4],'n_estimators':[3,10]}
]
fore_reg = RandomForestRegressor(random_state = 42)

grid_search = GridSearchCV(fore_reg, param_grid,cv = 5, scoring = 'neg_mean_squared_error',return_train_score = True)
grid_search.fit(train_prepared, y_train)

cvres = grid_search.cv_results_
for mean_score, params in zip(cvres["mean_test_score"], cvres["params"]):
     print(np.sqrt(-mean_score), params)

feature_importances = grid_search.best_estimator_.feature_importances_
feature_importances

final_model = grid_search.best_estimator_
final_model

final_predictions = final_model.predict(test_prepared)
final_mse = mean_squared_error(y_test, final_predictions)
final_rmse = np.sqrt(final_mse)

final_rmse

final_predictions=pd.Series(rand_for.predict(test_prepared))
final_pred=pd.DataFrame({'Id': test_df['Id'], 'price': final_predictions})